import numpy as np
import random
import logging
import warnings
from pathlib import Path
import gc
import pickle
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams

# ML Libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.multioutput import MultiOutputRegressor

# --- Matplotlib and Logging Configuration ---
# Set matplotlib font to a professional-looking serif font
plt.rcParams['font.family'] = 'DejaVu Serif'
plt.rcParams['font.size'] = 14
plt.rcParams['axes.labelsize'] = 16
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
plt.rcParams['legend.fontsize'] = 14
plt.rcParams['figure.titlesize'] = 20
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3
plt.rcParams['axes.facecolor'] = '#f9f9f9'
plt.rcParams['figure.facecolor'] = 'white'

# Custom color palette for plots
COLORS = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E', '#BC4B51',
          '#5D576B', '#F4A261', '#264653', '#E76F51']

# Suppress warnings for a cleaner output
warnings.filterwarnings('ignore')

# Setup logging to file and console with UTF-8 encoding
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('subsidence_model_rf_forecast.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RandomForestSubsidencePredictor:
    """
    A machine learning framework for single-step subsidence forecasting using a Random Forest model.
    - Trains, validates, and tests on separate, pre-defined NPZ data files.
    - Forecasts exactly one time step into the future.
    - Automatically finds the optimal number of past time steps for model input.
    - Generates professional visualizations for model performance and feature importance.
    """

    def __init__(self, train_data_path: str, val_data_path: str, test_data_path: str, random_state: int = 42):
        """
        Initialize the subsidence predictor.

        Args:
            train_data_path: Path to the training NPZ data file.
            val_data_path: Path to the validation NPZ data file.
            test_data_path: Path to the testing NPZ data file.
            random_state: Random state for reproducibility.
        """
        self.train_data_path = train_data_path
        self.val_data_path = val_data_path
        self.test_data_path = test_data_path
        self.random_state = random_state
        self.future_steps = 1  # Hardcoded to forecast one step forward

        # Set random seeds for reproducibility
        np.random.seed(random_state)
        random.seed(random_state)

        logger.info(f"Forecasting {self.future_steps} step(s) ahead.")

        # --- Feature Definitions ---
        # A comprehensive list of all available features in the dataset
        self.feature_names = [
            'Root Moisture (inst)', 'Soil Moisture 0-10cm (inst)', 'Soil Moisture 10-40cm (inst)',
            'Soil Moisture 40-100cm (inst)', 'Soil Moisture 100-200cm (inst)',
            'Soil Temperature 0-10cm (inst)', 'Soil Temperature 10-40cm (inst)',
            'Soil Temperature 40-100cm (inst)', 'Soil Temperature 100-200cm (inst)',
            'Evaporation (tavg)', 'Rainfall (tavg)', 'Subsurface Runoff (acc)',
            'Air Temperature (f inst)', 'Wind Speed (f inst)', 'Average Displacement (mm)',
            'Bulk Density Mean (0-5cm)', 'Cation Exchange Capacity Mean (0-5cm)',
            'Coarse Fragments Volumetric Mean (0-5cm)', 'Clay Content Mean (0-5cm)',
            'pH in Water Mean (0-5cm)', 'Sand Content Mean (0-5cm)', 'Silt Content Mean (0-5cm)',
            'Soil Organic Carbon Mean (0-5cm)', 'Bulk Density Mean (15-30cm)',
            'Cation Exchange Capacity Mean (15-30cm)', 'Coarse Fragments Volumetric Mean (15-30cm)',
            'Clay Content Mean (15-30cm)', 'pH in Water Mean (15-30cm)', 'Sand Content Mean (15-30cm)',
            'Silt Content Mean (15-30cm)', 'Soil Organic Carbon Mean (15-30cm)',
            'Bulk Density Mean (60-100cm)', 'Cation Exchange Capacity Mean (60-100cm)',
            'Coarse Fragments Volumetric Mean (60-100cm)', 'Clay Content Mean (60-100cm)',
            'pH in Water Mean (60-100cm)', 'Sand Content Mean (60-100cm)',
            'Silt Content Mean (60-100cm)', 'Soil Organic Carbon Mean (60-100cm)',
            'Bulk Density Mean (100-200cm)', 'Cation Exchange Capacity Mean (100-200cm)',
            'Coarse Fragments Volumetric Mean (100-200cm)', 'Clay Content Mean (100-200cm)',
            'pH in Water Mean (100-200cm)', 'Sand Content Mean (100-200cm)',
            'Silt Content Mean (100-200cm)', 'Soil Organic Carbon Mean (100-200cm)'
        ]
        # A curated list of features selected for the model based on prior analysis (e.g., mutual information)
        self.selected_feature_names = [
            'Sand Content Mean (15-30cm)', 'Silt Content Mean (60-100cm)',
            'Coarse Fragments Volumetric Mean (60-100cm)', 'Soil Organic Carbon Mean (15-30cm)',
            'Average Displacement (mm)', 'Root Moisture (inst)', 'Soil Temperature 10-40cm (inst)',
            'Evaporation (tavg)', 'Soil Moisture 0-10cm (inst)', 'Soil Moisture 10-40cm (inst)'
        ]
        self.target_name = 'Average Displacement (mm)'
        self.target_index = self.feature_names.index(self.target_name)

        # --- Model and Data Storage ---
        self.train_data, self.val_data, self.test_data = None, None, None
        self.model = None
        self.model_results = {}
        self.optimal_time_steps = 0
        self.feature_indices = []

        # Best hyperparameters for Random Forest (pre-defined)
        self.best_params = {
            'n_estimators': 300,
            'min_samples_split': 5,
            'min_samples_leaf': 1,
            'max_depth': None
        }

        # Create output directory for saving results
        self.output_dir = Path("random_forest_forecast_outputs")
        self.output_dir.mkdir(exist_ok=True)

    def _load_data_from_path(self, path: str) -> np.ndarray:
        """Helper function to load data from a single NPZ file."""
        try:
            logger.info(f"Loading data from: {path}")
            npz_file = np.load(path)
            data = npz_file['data']
            if len(data.shape) != 3:
                raise ValueError(f"Expected 3D data in {path}, but got {len(data.shape)}D")
            logger.info(f"  - Data shape: {data.shape}")
            return data
        except Exception as e:
            logger.error(f"Error loading data from {path}: {str(e)}")
            raise

    def load_datasets(self) -> None:
        """Load the training, validation, and test datasets from their respective files."""
        self.train_data = self._load_data_from_path(self.train_data_path)
        self.val_data = self._load_data_from_path(self.val_data_path)
        self.test_data = self._load_data_from_path(self.test_data_path)

        # Define feature indices based on the selected feature names
        self.feature_indices = [self.feature_names.index(name) for name in self.selected_feature_names]
        logger.info(f"Selected feature indices: {self.feature_indices}")

    def create_sequences(self, data: np.ndarray, past_months: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Create input sequences and corresponding targets from a given dataset.

        Args:
            data: The 3D numpy array (timesteps, grid_cells, features) to process.
            past_months: The number of past time steps to use as input.

        Returns:
            A tuple containing the input sequences (X) and target values (y).
        """
        sequences, targets = [], []
        num_timesteps, num_grid_cells, _ = data.shape

        for grid_idx in range(num_grid_cells):
            for t in range(past_months, num_timesteps - self.future_steps + 1):
                # Input sequence from the past
                seq = data[t - past_months:t, grid_idx, self.feature_indices]
                # Target is the displacement at the next time step
                target = data[t:t + self.future_steps, grid_idx, self.target_index]
                sequences.append(seq)
                targets.append(target)

        sequences, targets = np.array(sequences), np.array(targets)
        # Remove any sequences or targets containing NaN values
        valid_mask = ~(np.isnan(sequences).any(axis=(1, 2)) | np.isnan(targets).any(axis=1))
        
        return sequences[valid_mask], targets[valid_mask]

    def find_optimal_time_steps(self) -> int:
        """Find the optimal number of past time steps by evaluating on the validation set."""
        logger.info("Finding optimal number of past time steps for input...")
        results = []
        time_steps_range = range(2, 13)  # Test a range of 2 to 12 months

        for past_months in time_steps_range:
            try:
                # Create sequences for training and validation sets
                X_train, y_train = self.create_sequences(self.train_data, past_months)
                X_val, y_val = self.create_sequences(self.val_data, past_months)

                if len(X_train) < 100 or len(X_val) < 20:
                    logger.warning(f"Skipping {past_months} time steps due to insufficient samples.")
                    continue

                # Flatten sequences for the Random Forest model
                X_train_flat = X_train.reshape(X_train.shape[0], -1)
                X_val_flat = X_val.reshape(X_val.shape[0], -1)

                # Use a simpler, faster model for this evaluation
                model = RandomForestRegressor(n_estimators=50, random_state=self.random_state, n_jobs=-1)
                model.fit(X_train_flat, y_train.ravel())
                val_score = r2_score(y_val, model.predict(X_val_flat))

                results.append({'past_months': past_months, 'val_r2': val_score})
                logger.info(f"  - Time steps: {past_months}, Validation R²: {val_score:.4f}")

            except Exception as e:
                logger.error(f"Error while testing {past_months} time steps: {str(e)}")
                continue

        if not results:
            raise ValueError("Could not find a valid time step configuration. Check data quality and size.")

        best_result = max(results, key=lambda x: x['val_r2'])
        optimal_steps = best_result['past_months']
        logger.info(f"Optimal time steps found: {optimal_steps} (Validation R²: {best_result['val_r2']:.4f})")
        return optimal_steps

    def train_random_forest(self) -> None:
        """Train and evaluate the final Random Forest model using the optimal time steps."""
        self.optimal_time_steps = self.find_optimal_time_steps()
        logger.info(f"\nTraining final Random Forest with {self.optimal_time_steps} optimal time steps...")

        # Create final datasets using the optimal number of past months
        X_train, y_train = self.create_sequences(self.train_data, self.optimal_time_steps)
        X_val, y_val = self.create_sequences(self.val_data, self.optimal_time_steps)
        X_test, y_test = self.create_sequences(self.test_data, self.optimal_time_steps)

        # Flatten the data for Random Forest
        X_train_flat = X_train.reshape(X_train.shape[0], -1)
        X_val_flat = X_val.reshape(X_val.shape[0], -1)
        X_test_flat = X_test.reshape(X_test.shape[0], -1)
        
        # Since we predict one step, y is (n_samples, 1). We use .ravel() to make it (n_samples,)
        y_train_flat = y_train.ravel()
        y_val_flat = y_val.ravel()
        y_test_flat = y_test.ravel()

        # Initialize model with the pre-defined best hyperparameters
        self.model = RandomForestRegressor(
            random_state=self.random_state,
            n_jobs=-1,
            **self.best_params
        )
        logger.info(f"Using hyperparameters: {self.best_params}")

        # Train the model
        self.model.fit(X_train_flat, y_train_flat)

        # Make predictions
        y_train_pred = self.model.predict(X_train_flat)
        y_val_pred = self.model.predict(X_val_flat)
        y_test_pred = self.model.predict(X_test_flat)

        # Store results for analysis and plotting
        self.model_results = {
            'train': {'r2': r2_score(y_train_flat, y_train_pred), 'rmse': np.sqrt(mean_squared_error(y_train_flat, y_train_pred)), 'mae': mean_absolute_error(y_train_flat, y_train_pred)},
            'val': {'r2': r2_score(y_val_flat, y_val_pred), 'rmse': np.sqrt(mean_squared_error(y_val_flat, y_val_pred)), 'mae': mean_absolute_error(y_val_flat, y_val_pred)},
            'test': {'r2': r2_score(y_test_flat, y_test_pred), 'rmse': np.sqrt(mean_squared_error(y_test_flat, y_test_pred)), 'mae': mean_absolute_error(y_test_flat, y_test_pred)},
            'predictions': {
                'y_test_pred': y_test_pred,
                'y_test': y_test_flat,
            }
        }
        logger.info(f"Random Forest - Train R²: {self.model_results['train']['r2']:.4f}, Val R²: {self.model_results['val']['r2']:.4f}, Test R²: {self.model_results['test']['r2']:.4f}")

    def plot_performance_summary(self) -> None:
        """Plot the performance metrics (R², RMSE, MAE) for all data splits."""
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        metrics = ['r2', 'rmse', 'mae']
        metric_names = ['R² Score', 'RMSE (mm)', 'MAE (mm)']
        
        for idx, (metric, name) in enumerate(zip(metrics, metric_names)):
            ax = axes[idx]
            values = [self.model_results[split][metric] for split in ['train', 'val', 'test']]
            bars = ax.bar(['Train', 'Validation', 'Test'], values, color=COLORS[:3], alpha=0.8)
            ax.set_title(f'Random Forest - {name}', fontsize=16, fontweight='bold')
            ax.set_ylabel(name, fontsize=14)
            ax.grid(True, alpha=0.3, axis='y')

            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom', fontsize=12)

        plt.suptitle('Random Forest Performance Summary', fontsize=20, fontweight='bold')
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(self.output_dir / 'rf_performance_summary.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("  - Saved performance summary plot.")

    def plot_actual_vs_predicted(self) -> None:
        """Plot a scatter graph of actual vs. predicted values for the test set."""
        y_test = self.model_results['predictions']['y_test']
        y_test_pred = self.model_results['predictions']['y_test_pred']
        r2 = self.model_results['test']['r2']
        rmse = self.model_results['test']['rmse']
        mae = self.model_results['test']['mae']

        plt.figure(figsize=(10, 8))
        plt.scatter(y_test, y_test_pred, alpha=0.5, color=COLORS[0], edgecolors='k', s=50)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2.5, label='Ideal Fit')
        plt.xlabel('Actual Displacement (mm)', fontsize=14)
        plt.ylabel('Predicted Displacement (mm)', fontsize=14)
        plt.title(f'Actual vs. Predicted Displacement (Test Set)\nR² = {r2:.4f}', fontsize=16, fontweight='bold')
        textstr = f'RMSE: {rmse:.3f} mm\nMAE: {mae:.3f} mm'
        plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        plt.grid(True, alpha=0.3)
        plt.legend()
        plt.savefig(self.output_dir / 'rf_actual_vs_predicted.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("  - Saved actual vs. predicted plot.")

    def plot_prediction_residuals(self) -> None:
        """Plot the residuals of the predictions on the test set."""
        y_test = self.model_results['predictions']['y_test']
        y_test_pred = self.model_results['predictions']['y_test_pred']
        residuals = y_test - y_test_pred

        plt.figure(figsize=(12, 7))
        sns.histplot(residuals, kde=True, color=COLORS[4], bins=50)
        plt.title('Distribution of Prediction Residuals (Test Set)', fontsize=16, fontweight='bold')
        plt.xlabel('Residual (Actual - Predicted) (mm)', fontsize=14)
        plt.ylabel('Frequency', fontsize=14)
        plt.axvline(0, color='r', linestyle='--', lw=2)
        plt.grid(True, alpha=0.3)
        plt.savefig(self.output_dir / 'rf_prediction_residuals.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("  - Saved prediction residuals plot.")

    def plot_feature_importance(self) -> None:
        """Plot the most important features as determined by the trained model."""
        importances = self.model.feature_importances_
        
        # Create meaningful feature names including the time lag
        feature_names = [f't-{self.optimal_time_steps-t-1} {f.split("(")[0].strip()}' for t in range(self.optimal_time_steps) for f in self.selected_feature_names]
        indices = np.argsort(importances)[::-1][:20] # Top 20 features

        plt.figure(figsize=(12, 10))
        plt.title('Top 20 Feature Importances for Random Forest', fontsize=16, fontweight='bold')
        bars = plt.barh(range(len(indices)), importances[indices][::-1], color=COLORS[0], align='center')
        plt.yticks(range(len(indices)), [feature_names[i] for i in indices][::-1])
        plt.xlabel('Relative Importance', fontsize=14)
        
        for bar in bars:
            plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.3f}', ha='left', va='center')

        plt.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig(self.output_dir / 'rf_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("  - Saved feature importance plot.")
        
    def print_comprehensive_results(self) -> None:
        """Print a formatted, comprehensive summary of the final results to the console."""
        logger.info("\n" + "="*100)
        logger.info("RANDOM FOREST SUBSIDENCE FORECASTING RESULTS")
        logger.info("="*100)
        logger.info(f"Training Data:   {self.train_data_path}")
        logger.info(f"Validation Data: {self.val_data_path}")
        logger.info(f"Test Data:       {self.test_data_path}")
        logger.info(f"Time steps forecasted: {self.future_steps}")
        logger.info(f"Optimal past time steps used for input: {self.optimal_time_steps}")
        logger.info(f"Model Hyperparameters: {self.best_params}")
        logger.info("="*100)

        metrics = self.model_results
        logger.info("\nMODEL PERFORMANCE SUMMARY:")
        logger.info("-" * 50)
        logger.info(f"{'Split':<12}{'R² Score':<12}{'RMSE (mm)':<12}{'MAE (mm)':<12}")
        logger.info("-" * 50)
        logger.info(f"{'Training':<12}{metrics['train']['r2']:<12.4f}{metrics['train']['rmse']:<12.4f}{metrics['train']['mae']:<12.4f}")
        logger.info(f"{'Validation':<12}{metrics['val']['r2']:<12.4f}{metrics['val']['rmse']:<12.4f}{metrics['val']['mae']:<12.4f}")
        logger.info(f"{'Test':<12}{metrics['test']['r2']:<12.4f}{metrics['test']['rmse']:<12.4f}{metrics['test']['mae']:<12.4f}")
        logger.info("="*100)

    def save_model_and_results(self) -> None:
        """Save the trained model and a dictionary of results to disk using pickle."""
        # Save the trained model object
        with open(self.output_dir / 'random_forest_model.pickle', 'wb') as f:
            pickle.dump(self.model, f)
            
        # Save a dictionary containing key results and configuration
        results_to_save = {
            'model_results': self.model_results,
            'optimal_time_steps': self.optimal_time_steps,
            'best_hyperparameters': self.best_params,
            'selected_features': self.selected_feature_names,
            'future_steps_forecasted': self.future_steps
        }
        with open(self.output_dir / 'random_forest_results.pickle', 'wb') as f:
            pickle.dump(results_to_save, f)
            
        logger.info(f"Model and results dictionary saved to '{self.output_dir}/'")

    def run_complete_pipeline(self) -> None:
        """Execute the complete machine learning pipeline from data loading to saving results."""
        logger.info("Starting Random Forest Subsidence Forecasting Pipeline...")
        try:
            self.load_datasets()
            self.train_random_forest()
            
            logger.info("\nCreating and saving visualizations...")
            self.plot_performance_summary()
            self.plot_actual_vs_predicted()
            self.plot_prediction_residuals()
            self.plot_feature_importance()

            self.print_comprehensive_results()
            self.save_model_and_results()

            logger.info("\nPipeline completed successfully!")
            logger.info(f"All outputs have been saved to: {self.output_dir.absolute()}")

        except Exception as e:
            logger.error(f"The pipeline failed with an error: {str(e)}", exc_info=True)
            raise


# --- Main Execution Block ---
if __name__ == "__main__":
    # --- Configuration ---
    # Define the paths for the training, validation, and test datasets.
    # IMPORTANT: Update these paths to the correct locations of your NPZ files.
    TRAIN_DATA_PATH = "/content/URM_TEH_NEY_MARV_LENJ.npz"
    VAL_DATA_PATH = "/content/BARD.npz"
    TEST_DATA_PATH = "/content/SEMN_DOLAT.npz"
    
    RANDOM_STATE = 42

    # Create an instance of the predictor and run the pipeline
    predictor = RandomForestSubsidencePredictor(
        train_data_path=TRAIN_DATA_PATH,
        val_data_path=VAL_DATA_PATH,
        test_data_path=TEST_DATA_PATH,
        random_state=RANDOM_STATE
    )
    
    predictor.run_complete_pipeline()
