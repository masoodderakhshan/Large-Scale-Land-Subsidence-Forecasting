import numpy as np
import pandas as pd
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.inspection import permutation_importance
import shap
import joblib
import warnings

# Suppress warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# --- MODIFIED: Define data paths ---
TRAIN_DATA_PATH = "/content/URM_TEH_NEY_MARV_LENJ.npz"
VAL_DATA_PATH = "/content/BARD.npz"
TEST_DATA_PATH = "/content/SEMN_DOLAT.npz"

# --- MODIFIED: Load the separate datasets ---
train_loaded = np.load(TRAIN_DATA_PATH)
X_train_full, coords_train = train_loaded['data'], train_loaded['coords']

val_loaded = np.load(VAL_DATA_PATH)
X_val_full, coords_val = val_loaded['data'], val_loaded['coords']

test_loaded = np.load(TEST_DATA_PATH)
X_test_full, coords_test = test_loaded['data'], test_loaded['coords']


# Parameter names (remains the same)
param_names = [
    'RootMoist_inst', 'SoilMoi0_10cm_inst', 'SoilMoi10_40cm_inst',
    'SoilMoi40_100cm_inst', 'SoilMoi100_200cm_inst', 'SoilTMP0_10cm_inst',
    'SoilTMP10_40cm_inst', 'SoilTMP40_100cm_inst', 'SoilTMP100_200cm_inst',
    'Evap_tavg', 'Rainf_tavg', 'Qsb_acc', 'Tair_f_inst', 'Wind_f_inst',
    'avg_displacement_mm', 'bdod_mean_05', 'cec_mean_05', 'cfvo_mean_05',
    'clay_mean_05', 'phh2o_mean_05', 'sand_mean_05', 'silt_mean_05',
    'soc_mean_05', 'bdod_mean_1530', 'cec_mean_1530', 'cfvo_mean_1530',
    'clay_mean_1530', 'phh2o_mean_1530', 'sand_mean_1530', 'silt_mean_1530',
    'soc_mean_1530', 'bdod_mean_60100', 'cec_mean_60100', 'cfvo_mean_60100',
    'clay_mean_60100', 'phh2o_mean_60100', 'sand_mean_60100', 'silt_mean_60100',
    'soc_mean_60100', 'bdod_mean_100200', 'cec_mean_100200', 'cfvo_mean_100200',
    'clay_mean_100200', 'phh2o_mean_100200', 'sand_mean_100200',
    'silt_mean_100200', 'soc_mean_100200'
]

def prepare_data(X_full, coords, param_names):
    """Prepare the data frame from 3D array"""
    n_time, n_locations, n_params = X_full.shape
    
    # Reshape data and create DataFrame
    X_reshaped = X_full.reshape(-1, n_params)
    df = pd.DataFrame(X_reshaped, columns=param_names)
    
    # Add metadata
    df['location_id'] = np.repeat(np.arange(n_locations), n_time)
    df['time_step'] = np.tile(np.arange(n_time), n_locations)
    
    # Add coordinates
    # Correcting coordinate expansion for this structure
    coords_expanded = np.repeat(coords, n_time, axis=0)
    df['lat'] = coords_expanded[:, 0]
    df['lon'] = coords_expanded[:, 1]
    
    # Sort by location and time
    df_sorted = df.sort_values(['location_id', 'time_step'])
    
    # Create target (next time step's displacement)
    df_sorted['target_displacement'] = df_sorted.groupby('location_id')['avg_displacement_mm'].shift(-1)
    
    # Drop rows with missing values (including the last time step which has no target)
    df_final = df_sorted.dropna().reset_index(drop=True)
    
    return df_final

# --- MODIFIED: Prepare each dataset ---
df_train = prepare_data(X_train_full, coords_train, param_names)
df_val = prepare_data(X_val_full, coords_val, param_names)
df_test = prepare_data(X_test_full, coords_test, param_names)

def add_temporal_features(df):
    """Add temporal features to help with time-series prediction"""
    # Rolling statistics for displacement
    for window in [3, 6]:  # 1.5 and 3 month windows (bi-monthly data)
        df[f'disp_rolling_mean_{window}'] = df.groupby('location_id')['avg_displacement_mm'].transform(
            lambda x: x.rolling(window=window, min_periods=1).mean()
        )
        df[f'disp_rolling_std_{window}'] = df.groupby('location_id')['avg_displacement_mm'].transform(
            lambda x: x.rolling(window=window, min_periods=1).std()
        )
    
    # Time since last measurement
    df['time_since_last'] = df.groupby('location_id')['time_step'].diff()
    
    # Fill initial NaNs from rolling/diff with 0 or forward fill
    df.fillna(0, inplace=True)
    
    return df

# --- MODIFIED: Add temporal features to each dataset ---
df_train = add_temporal_features(df_train)
df_val = add_temporal_features(df_val)
df_test = add_temporal_features(df_test)


# --- MODIFIED: Prepare features and target for each set ---
features_to_drop = ['avg_displacement_mm', 'target_displacement', 'location_id', 'lat', 'lon', 'time_step']

X_train = df_train.drop(columns=features_to_drop)
y_train = df_train['target_displacement']

X_val = df_val.drop(columns=features_to_drop)
y_val = df_val['target_displacement']

X_test = df_test.drop(columns=features_to_drop)
y_test = df_test['target_displacement']

# Align columns to ensure consistency, in case some features are all NaN in a split
train_cols = X_train.columns
X_val = X_val[train_cols]
X_test = X_test[train_cols]

# Time-series cross-validation for hyperparameter tuning
tscv = TimeSeriesSplit(n_splits=5)

# Parameter grid for tuning
param_grid = {
    'n_estimators': [200, 300],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'max_features': ['sqrt', 0.5]
}

# Initialize and tune the model on the TRAINING data
print("--- Tuning model on training data ---")
model_all = GridSearchCV(
    RandomForestRegressor(random_state=42, n_jobs=-1),
    param_grid,
    cv=tscv,
    scoring='neg_mean_squared_error',
    verbose=1,
    n_jobs=-1
)
model_all.fit(X_train, y_train)

# Save the best model
joblib.dump(model_all.best_estimator_, 'displacement_model.pkl')

def analyze_features(model, X, y):
    """Perform comprehensive feature importance analysis"""
    # Get feature importances from model
    importances = model.feature_importances_
    feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': importances})
    feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)
    
    # Plot top 20 features
    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=feature_importance_df.head(20))
    plt.title('Top 20 Feature Importances (MDI)')
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
    
    # Permutation importance
    print("Calculating permutation importance...")
    result = permutation_importance(
        model, X, y, n_repeats=10, random_state=42, n_jobs=-1
    )
    
    perm_importance_df = pd.DataFrame({
        'feature': X.columns,
        'importance': result.importances_mean,
        'std': result.importances_std
    }).sort_values('importance', ascending=False)
    
    # Plot permutation importance
    plt.figure(figsize=(12, 8))
    sns.barplot(x='importance', y='feature', data=perm_importance_df.head(20))
    plt.title('Top 20 Permutation Importances')
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
    
    # SHAP values
    print("Calculating SHAP values...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)
    
    shap.summary_plot(shap_values, X, plot_type="bar", max_display=20, show=False)
    plt.title("SHAP Feature Importance")
    plt.tight_layout()
    plt.show()
    
    shap.summary_plot(shap_values, X, max_display=20, show=False)
    plt.title("SHAP Feature Contribution")
    plt.tight_layout()
    plt.show()
    
    return feature_importance_df, perm_importance_df

# --- MODIFIED: Analyze features on the TRAINING data ---
print("\n--- Analyzing features on training data ---")
mdi_importance, perm_importance = analyze_features(model_all.best_estimator_, X_train, y_train)

# Select top features based on consensus
top_features = list(set(mdi_importance.head(20)['feature']).intersection(
    set(perm_importance.head(20)['feature'])))
top_features = sorted(top_features,
                      key=lambda x: perm_importance.set_index('feature').loc[x, 'importance'],
                      reverse=True)

print("\nTop 15 consensus features:")
print(top_features[:15])

# --- MODIFIED: Create feature-selected datasets ---
X_train_top = X_train[top_features[:15]]
X_val_top = X_val[top_features[:15]]
X_test_top = X_test[top_features[:15]]

# Train final model with selected features on TRAINING data
print("\n--- Training final model on selected features ---")
final_model = RandomForestRegressor(**model_all.best_params_, random_state=42, n_jobs=-1)
final_model.fit(X_train_top, y_train)

# Save final model
joblib.dump(final_model, 'displacement_model_final.pkl')

# --- NEW: Evaluation function for hold-out sets ---
def evaluate_on_holdout(full_model, final_model, X_full, y_full, X_top, y_top, set_name=""):
    """Evaluate trained models on a holdout dataset."""
    print(f"\n--- {set_name} Set Evaluation ---")
    
    # Full Model
    pred_full = full_model.predict(X_full)
    mse_full = mean_squared_error(y_full, pred_full)
    r2_full = r2_score(y_full, pred_full)
    print(f"Full Model MSE: {mse_full:.4f}")
    print(f"Full Model R²: {r2_full:.4f}")
    
    # Final Model (Top Features)
    pred_final = final_model.predict(X_top)
    mse_final = mean_squared_error(y_top, pred_final)
    r2_final = r2_score(y_top, pred_final)
    print(f"Final Model MSE: {mse_final:.4f}")
    print(f"Final Model R²: {r2_final:.4f}")

# --- MODIFIED: Evaluate on validation and test sets ---
full_model_best = model_all.best_estimator_
evaluate_on_holdout(full_model_best, final_model, X_val, y_val, X_val_top, y_val, "Validation")
evaluate_on_holdout(full_model_best, final_model, X_test, y_test, X_test_top, y_test, "Test")


# Feature correlation on the training data
plt.figure(figsize=(12, 10))
corr_matrix = X_train_top.corr()
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', center=0)
plt.title('Correlation Matrix of Selected Features (from Training Data)')
plt.tight_layout()
plt.show()

# --- MODIFIED: Time-series predictions for sample locations from the TEST set ---
print("\n--- Plotting predictions on test data samples ---")
sample_locations = df_test['location_id'].unique()[:3]  # First 3 locations in the test set

for loc in sample_locations:
    loc_data = df_test[df_test['location_id'] == loc]
    
    X_loc_full = loc_data[train_cols] # Ensure columns are in the same order as training
    X_loc_top = loc_data[top_features[:15]]
    
    preds_full = full_model_best.predict(X_loc_full)
    preds_final = final_model.predict(X_loc_top)
    
    plt.figure(figsize=(12, 6))
    plt.plot(loc_data['time_step'], loc_data['target_displacement'], label='Actual', marker='o')
    plt.plot(loc_data['time_step'], preds_full, label='Full Model Predicted', linestyle='--')
    plt.plot(loc_data['time_step'], preds_final, label='Final Model Predicted', linestyle='-.')
    plt.title(f'Displacement Prediction for Test Location {loc}')
    plt.xlabel('Time Step')
    plt.ylabel('Displacement (mm)')
    plt.legend()
    plt.grid(True)
    plt.show()