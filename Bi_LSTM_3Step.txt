import numpy as np
import random
import logging
import warnings
from pathlib import Path
import gc
import pickle
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams

# ML Libraries
# --- Switched from LightGBM to TensorFlow/Keras for the BiLSTM model ---
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import MinMaxScaler

# --- Matplotlib and Logging Configuration (unchanged) ---
plt.rcParams['font.family'] = 'DejaVu Serif'
plt.rcParams['font.size'] = 14
plt.rcParams['axes.labelsize'] = 16
plt.rcParams['axes.titlesize'] = 18
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
plt.rcParams['legend.fontsize'] = 14
plt.rcParams['figure.titlesize'] = 20
plt.rcParams['axes.grid'] = True
plt.rcParams['grid.alpha'] = 0.3
plt.rcParams['axes.facecolor'] = '#f9f9f9'
plt.rcParams['figure.facecolor'] = 'white'

COLORS = ['#0077B6', '#D9534F', '#5CB85C', '#F0AD4E', '#5BC0DE', '#428BCA']
warnings.filterwarnings('ignore')

# --- Configure logging for the new BiLSTM model ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('subsidence_model_bilstm_multistep_forecast.log', 'w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- Renamed class to reflect the change from LGBM to BiLSTM ---
class BiLSTMSubsidencePredictor:
    """
    A machine learning framework for multi-step subsidence forecasting using a
    Bidirectional LSTM (BiLSTM) model.
    """

    def __init__(self, train_data_path: str, val_data_path: str, test_data_path: str, random_state: int = 42):
        self.train_data_path = train_data_path
        self.val_data_path = val_data_path
        self.test_data_path = test_data_path
        self.random_state = random_state
        self.future_steps = 3  # Forecast three steps forward

        # --- Set seeds for reproducibility with TensorFlow ---
        np.random.seed(random_state)
        random.seed(random_state)
        tf.random.set_seed(random_state)
        logger.info(f"Forecasting {self.future_steps} step(s) ahead using BiLSTM.")

        # --- Feature names and target configuration (unchanged) ---
        self.feature_names = [
            'Root Moisture (inst)', 'Soil Moisture 0-10cm (inst)', 'Soil Moisture 10-40cm (inst)',
            'Soil Moisture 40-100cm (inst)', 'Soil Moisture 100-200cm (inst)',
            'Soil Temperature 0-10cm (inst)', 'Soil Temperature 10-40cm (inst)',
            'Soil Temperature 40-100cm (inst)', 'Soil Temperature 100-200cm (inst)',
            'Evaporation (tavg)', 'Rainfall (tavg)', 'Subsurface Runoff (acc)',
            'Air Temperature (f inst)', 'Wind Speed (f inst)', 'Average Displacement (mm)',
            'Bulk Density Mean (0-5cm)', 'Cation Exchange Capacity Mean (0-5cm)',
            'Coarse Fragments Volumetric Mean (0-5cm)', 'Clay Content Mean (0-5cm)',
            'pH in Water Mean (0-5cm)', 'Sand Content Mean (0-5cm)', 'Silt Content Mean (0-5cm)',
            'Soil Organic Carbon Mean (0-5cm)', 'Bulk Density Mean (15-30cm)',
            'Cation Exchange Capacity Mean (15-30cm)', 'Coarse Fragments Volumetric Mean (15-30cm)',
            'Clay Content Mean (15-30cm)', 'pH in Water Mean (15-30cm)', 'Sand Content Mean (15-30cm)',
            'Silt Content Mean (15-30cm)', 'Soil Organic Carbon Mean (15-30cm)',
            'Bulk Density Mean (60-100cm)', 'Cation Exchange Capacity Mean (60-100cm)',
            'Coarse Fragments Volumetric Mean (60-100cm)', 'Clay Content Mean (60-100cm)',
            'pH in Water Mean (60-100cm)', 'Sand Content Mean (60-100cm)',
            'Silt Content Mean (60-100cm)', 'Soil Organic Carbon Mean (60-100cm)',
            'Bulk Density Mean (100-200cm)', 'Cation Exchange Capacity Mean (100-200cm)',
            'Coarse Fragments Volumetric Mean (100-200cm)', 'Clay Content Mean (100-200cm)',
            'pH in Water Mean (100-200cm)', 'Sand Content Mean (100-200cm)',
            'Silt Content Mean (100-200cm)', 'Soil Organic Carbon Mean (100-200cm)'
        ]
        self.selected_feature_names = [
            'Sand Content Mean (15-30cm)', 'Silt Content Mean (60-100cm)',
            'Coarse Fragments Volumetric Mean (60-100cm)', 'Soil Organic Carbon Mean (15-30cm)',
            'Average Displacement (mm)', 'Root Moisture (inst)', 'Soil Temperature 10-40cm (inst)',
            'Evaporation (tavg)', 'Soil Moisture 0-10cm (inst)', 'Soil Moisture 10-40cm (inst)'
        ]
        self.target_name = 'Average Displacement (mm)'
        self.target_index = self.feature_names.index(self.target_name)

        # --- Initialize model-related attributes ---
        self.train_data, self.val_data, self.test_data = None, None, None
        self.model: Optional[Model] = None
        self.history: Optional[Dict[str, List[float]]] = None
        self.model_results: Dict[str, Any] = {}
        self.optimal_time_steps = 0
        self.feature_indices: List[int] = []
        self.scalers: Dict[int, MinMaxScaler] = {} # Scalers for features

        # --- Define output directory for BiLSTM results ---
        self.output_dir = Path("bilstm_multistep_forecast_outputs")
        self.output_dir.mkdir(exist_ok=True)

    def _load_data_from_path(self, path: str) -> np.ndarray:
        """Loads data from a .npz file."""
        try:
            logger.info(f"Loading data from: {path}")
            data = np.load(path)['data']
            logger.info(f"  - Data shape: {data.shape}")
            return data
        except Exception as e:
            logger.error(f"Error loading data from {path}: {str(e)}")
            raise

    def load_and_scale_datasets(self) -> None:
        """Loads all datasets and applies scaling."""
        self.train_data = self._load_data_from_path(self.train_data_path)
        self.val_data = self._load_data_from_path(self.val_data_path)
        self.test_data = self._load_data_from_path(self.test_data_path)
        self.feature_indices = [self.feature_names.index(name) for name in self.selected_feature_names]

        # --- Scale features for better neural network performance ---
        logger.info("Scaling data using MinMaxScaler...")
        for i in self.feature_indices:
            scaler = MinMaxScaler(feature_range=(-1, 1))
            # Reshape to (timesteps * grid_cells, 1) to fit the scaler
            self.train_data[:, :, i] = scaler.fit_transform(self.train_data[:, :, i].reshape(-1, 1)).reshape(self.train_data.shape[0], self.train_data.shape[1])
            self.val_data[:, :, i] = scaler.transform(self.val_data[:, :, i].reshape(-1, 1)).reshape(self.val_data.shape[0], self.val_data.shape[1])
            self.test_data[:, :, i] = scaler.transform(self.test_data[:, :, i].reshape(-1, 1)).reshape(self.test_data.shape[0], self.test_data.shape[1])
            self.scalers[i] = scaler
        logger.info("Data scaling complete.")


    def create_sequences(self, data: np.ndarray, past_months: int) -> Tuple[np.ndarray, np.ndarray]:
        """Creates sequences and targets for the LSTM model (unchanged)."""
        sequences, targets = [], []
        num_timesteps, num_grid_cells, _ = data.shape
        for grid_idx in range(num_grid_cells):
            for t in range(past_months, num_timesteps - self.future_steps + 1):
                seq = data[t - past_months:t, grid_idx, self.feature_indices]
                target = data[t:t + self.future_steps, grid_idx, self.target_index]
                sequences.append(seq)
                targets.append(target)
        sequences, targets = np.array(sequences), np.array(targets)
        valid_mask = ~(np.isnan(sequences).any(axis=(1, 2)) | np.isnan(targets).any(axis=1))
        return sequences[valid_mask], targets[valid_mask]

    def _build_simple_lstm_for_search(self, input_shape: Tuple[int, int]) -> Model:
        """A very simple model for quickly finding the optimal time steps."""
        model = Sequential([
            Input(shape=input_shape),
            LSTM(16, activation='relu'),
            Dense(self.future_steps)
        ])
        model.compile(optimizer='adam', loss='mse')
        return model

    def find_optimal_time_steps(self) -> int:
        """
        Finds the optimal number of past time steps.
        Note: Training a neural network in a loop is slow. This uses a very simple
        model and few epochs for demonstration.
        """
        logger.info("Finding optimal number of past time steps for input...")
        results = []
        for past_months in range(2, 13):
            try:
                X_train, y_train = self.create_sequences(self.train_data, past_months)
                if len(X_train) < 100: continue

                X_val, y_val = self.create_sequences(self.val_data, past_months)
                if len(X_val) < 20: continue

                # --- Use a simplified LSTM for the search to speed it up ---
                model = self._build_simple_lstm_for_search(input_shape=(X_train.shape[1], X_train.shape[2]))
                model.fit(X_train, y_train, epochs=5, batch_size=256, verbose=0)

                val_preds = model.predict(X_val)
                val_score = r2_score(y_val, val_preds)
                results.append({'past_months': past_months, 'val_r2': val_score})
                logger.info(f"  - Time steps: {past_months}, Validation R² (avg): {val_score:.4f}")
                gc.collect() # Clean up memory
            except Exception as e:
                logger.error(f"Error while testing {past_months} time steps: {str(e)}")
                continue

        if not results:
            logger.warning("Could not find optimal time steps. Defaulting to 6.")
            return 6
        best_result = max(results, key=lambda x: x['val_r2'])
        logger.info(f"Optimal time steps found: {best_result['past_months']} (R²: {best_result['val_r2']:.4f})")
        return best_result['past_months']

    def _build_bilstm_model(self, input_shape: Tuple[int, int]) -> Model:
        """Builds the BiLSTM model architecture."""
        logger.info("Building the BiLSTM model...")
        model = Sequential([
            Input(shape=input_shape, name='input_layer'),
            Bidirectional(LSTM(128, return_sequences=True, activation='tanh'), name='bilstm_1'),
            Dropout(0.3, name='dropout_1'),
            Bidirectional(LSTM(64, activation='tanh'), name='bilstm_2'),
            Dropout(0.3, name='dropout_2'),
            Dense(32, activation='relu', name='dense_layer'),
            Dense(self.future_steps, name='output_layer') # Output layer for multi-step forecast
        ])
        
        optimizer = Adam(learning_rate=0.001)
        model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])
        model.summary(print_fn=logger.info)
        return model

    def train_bilstm_model(self) -> None:
        """
        Prepares data, builds, trains, and evaluates the BiLSTM model.
        This method replaces the `train_lgbm_model`.
        """
        self.optimal_time_steps = self.find_optimal_time_steps()
        logger.info(f"\nTraining final BiLSTM with {self.optimal_time_steps} optimal time steps...")

        X_train, y_train = self.create_sequences(self.train_data, self.optimal_time_steps)
        X_val, y_val = self.create_sequences(self.val_data, self.optimal_time_steps)
        X_test, y_test = self.create_sequences(self.test_data, self.optimal_time_steps)

        logger.info(f"Train sequences shape: {X_train.shape}")
        logger.info(f"Validation sequences shape: {X_val.shape}")
        logger.info(f"Test sequences shape: {X_test.shape}")
        
        self.model = self._build_bilstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))
        
        # --- Callbacks for training ---
        model_checkpoint_path = self.output_dir / 'best_bilstm_model.keras'
        checkpoint = ModelCheckpoint(
            filepath=model_checkpoint_path,
            save_best_only=True,
            monitor='val_loss',
            mode='min',
            verbose=1
        )
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=15, # Stop if val_loss doesn't improve for 15 epochs
            restore_best_weights=True,
            mode='min'
        )

        logger.info("Starting model training...")
        self.history = self.model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=128,
            validation_data=(X_val, y_val),
            callbacks=[checkpoint, early_stopping],
            verbose=1
        ).history

        # --- Load the best model saved by ModelCheckpoint ---
        logger.info(f"Loading best model from {model_checkpoint_path}")
        self.model = tf.keras.models.load_model(model_checkpoint_path)

        # --- Evaluate the final model ---
        logger.info("Evaluating model performance...")
        y_train_pred = self.model.predict(X_train)
        y_val_pred = self.model.predict(X_val)
        y_test_pred = self.model.predict(X_test)
        
        # --- Inverse transform predictions and true values to original scale ---
        target_scaler = self.scalers[self.target_index]
        y_train_inv = target_scaler.inverse_transform(y_train)
        y_val_inv = target_scaler.inverse_transform(y_val)
        y_test_inv = target_scaler.inverse_transform(y_test)
        y_train_pred_inv = target_scaler.inverse_transform(y_train_pred)
        y_val_pred_inv = target_scaler.inverse_transform(y_val_pred)
        y_test_pred_inv = target_scaler.inverse_transform(y_test_pred)

        self.model_results = {
            'train': {'r2': r2_score(y_train_inv, y_train_pred_inv), 'rmse': np.sqrt(mean_squared_error(y_train_inv, y_train_pred_inv)), 'mae': mean_absolute_error(y_train_inv, y_train_pred_inv)},
            'val': {'r2': r2_score(y_val_inv, y_val_pred_inv), 'rmse': np.sqrt(mean_squared_error(y_val_inv, y_val_pred_inv)), 'mae': mean_absolute_error(y_val_inv, y_val_pred_inv)},
            'test': {'r2': r2_score(y_test_inv, y_test_pred_inv), 'rmse': np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv)), 'mae': mean_absolute_error(y_test_inv, y_test_pred_inv)},
            'predictions': {'y_test_pred': y_test_pred_inv, 'y_test': y_test_inv}
        }
        logger.info(f"BiLSTM - Train R² (avg): {self.model_results['train']['r2']:.4f}, Val R² (avg): {self.model_results['val']['r2']:.4f}, Test R² (avg): {self.model_results['test']['r2']:.4f}")

    def plot_training_history(self) -> None:
        """
        Plots the training and validation loss and MAE over epochs.
        This replaces the SHAP plot, as SHAP is not suitable for LSTMs.
        """
        if not self.history:
            logger.warning("No training history found. Skipping plot.")
            return

        logger.info("Plotting training history...")
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))

        # Plot Loss
        ax1.plot(self.history['loss'], label='Train Loss', color=COLORS[0], lw=2)
        ax1.plot(self.history['val_loss'], label='Validation Loss', color=COLORS[1], lw=2, linestyle='--')
        ax1.set_title('Model Loss Over Epochs', fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Mean Squared Error (Loss)')
        ax1.legend()
        ax1.grid(True)

        # Plot MAE
        ax2.plot(self.history['mae'], label='Train MAE', color=COLORS[2], lw=2)
        ax2.plot(self.history['val_mae'], label='Validation MAE', color=COLORS[3], lw=2, linestyle='--')
        ax2.set_title('Model MAE Over Epochs', fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Mean Absolute Error')
        ax2.legend()
        ax2.grid(True)

        plt.suptitle('BiLSTM Model Training History', fontsize=20, fontweight='bold')
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.savefig(self.output_dir / 'bilstm_training_history.png', dpi=400, bbox_inches='tight')
        plt.close()
        logger.info("  - Saved training history plot.")


    def plot_performance_summary(self) -> None:
        """Plots the final performance metrics for train, val, and test sets."""
        fig, axes = plt.subplots(1, 3, figsize=(20, 6))
        metrics = ['r2', 'rmse', 'mae']
        metric_names = ['R² Score (Avg)', 'RMSE (mm, Avg)', 'MAE (mm, Avg)']
        
        for idx, (metric, name) in enumerate(zip(metrics, metric_names)):
            ax = axes[idx]
            values = [self.model_results[split][metric] for split in ['train', 'val', 'test']]
            bars = ax.bar(['Train', 'Validation', 'Test'], values, color=COLORS[:3], alpha=0.8)
            ax.set_title(f'BiLSTM - {name}', fontsize=16, fontweight='bold')
            ax.set_ylabel(name, fontsize=14)
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.3f}', ha='center', va='bottom', fontsize=12)

        plt.suptitle('BiLSTM Multi-Step Performance Summary', fontsize=20, fontweight='bold')
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.savefig(self.output_dir / 'bilstm_multistep_performance_summary.png', dpi=400, bbox_inches='tight')
        plt.close()
        logger.info("  - Saved performance summary plot.")
        
    def run_complete_pipeline(self) -> None:
        """Executes the entire forecasting pipeline."""
        logger.info("Starting BiLSTM Multi-Step Subsidence Forecasting Pipeline...")
        try:
            self.load_and_scale_datasets()
            self.train_bilstm_model()
            logger.info("\nCreating and saving visualizations...")
            self.plot_performance_summary()
            self.plot_training_history() # Replaced SHAP plot
        except Exception as e:
            logger.error(f"The pipeline failed with an error: {str(e)}", exc_info=True)
            raise

# --- Main Execution Block ---
if __name__ == "__main__":
    # --- Ensure you have the data files in the correct path ---
    # Note: You might need to upload these files to your environment if using a cloud notebook.
    TRAIN_DATA_PATH = "/content/URM_TEH_NEY_MARV_LENJ.npz"
    VAL_DATA_PATH = "/content/BARD.npz"
    TEST_DATA_PATH = "/content/SEMN_DOLAT.npz"
    RANDOM_STATE = 42

    # --- Check if data files exist before running ---
    if not all(Path(p).exists() for p in [TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH]):
        logger.error("One or more data files not found. Please check the paths.")
        # Create dummy files for demonstration if they don't exist
        logger.info("Creating dummy data files for demonstration purposes...")
        for path_str in [TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH]:
            path = Path(path_str)
            path.parent.mkdir(exist_ok=True, parents=True)
            # Create data with shape (timesteps, grid_cells, features)
            # e.g., 100 timesteps, 5 grid cells, 51 features
            dummy_data = np.random.rand(100, 5, 51) 
            np.savez_compressed(path, data=dummy_data)
        logger.info("Dummy files created. The model will run with random data.")

    predictor = BiLSTMSubsidencePredictor(
        train_data_path=TRAIN_DATA_PATH,
        val_data_path=VAL_DATA_PATH,
        test_data_path=TEST_DATA_PATH,
        random_state=RANDOM_STATE
    )
    predictor.run_complete_pipeline()

